"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.LOCAL_APPS = void 0;
const gguf_js_1 = require("./gguf.js");
const common_js_1 = require("./snippets/common.js");
const inputs_js_1 = require("./snippets/inputs.js");
function isAwqModel(model) {
    return model.config?.quantization_config?.quant_method === "awq";
}
function isGptqModel(model) {
    return model.config?.quantization_config?.quant_method === "gptq";
}
function isAqlmModel(model) {
    return model.config?.quantization_config?.quant_method === "aqlm";
}
function isMarlinModel(model) {
    return model.config?.quantization_config?.quant_method === "marlin";
}
function isTransformersModel(model) {
    return model.tags.includes("transformers");
}
function isTgiModel(model) {
    return model.tags.includes("text-generation-inference");
}
function isLlamaCppGgufModel(model) {
    return !!model.gguf?.context_length;
}
function isMlxModel(model) {
    return model.tags.includes("mlx");
}
const snippetLlamacpp = (model, filepath) => {
    let tagName = "";
    if (filepath) {
        const quantLabel = (0, gguf_js_1.parseGGUFQuantLabel)(filepath);
        tagName = quantLabel ? `:${quantLabel}` : "";
    }
    const command = (binary) => {
        const snippet = ["# Load and run the model:", `${binary} -hf ${model.id}${tagName}`];
        if (!model.tags.includes("conversational")) {
            // for non-conversational models, add a prompt
            snippet[snippet.length - 1] += " \\";
            snippet.push('  -p "Once upon a time,"');
        }
        return snippet.join("\n");
    };
    return [
        {
            title: "Install from brew",
            setup: "brew install llama.cpp",
            content: command("llama-cli"),
        },
        {
            title: "Use pre-built binary",
            setup: [
                // prettier-ignore
                "# Download pre-built binary from:",
                "# https://github.com/ggerganov/llama.cpp/releases",
            ].join("\n"),
            content: command("./llama-cli"),
        },
        {
            title: "Build from source code",
            setup: [
                "git clone https://github.com/ggerganov/llama.cpp.git",
                "cd llama.cpp",
                "cmake -B build -DLLAMA_CURL=ON",
                "cmake --build build -j --target llama-cli",
            ].join("\n"),
            content: command("./build/bin/llama-cli"),
        },
    ];
};
const snippetNodeLlamaCppCli = (model, filepath) => {
    let tagName = "{{OLLAMA_TAG}}";
    if (filepath) {
        const quantLabel = (0, gguf_js_1.parseGGUFQuantLabel)(filepath);
        tagName = quantLabel ? `:${quantLabel}` : tagName;
    }
    return [
        {
            title: "Chat with the model",
            content: `npx -y node-llama-cpp chat hf:${model.id}${tagName}`,
        },
        {
            title: "Estimate the model compatibility with your hardware",
            content: `npx -y node-llama-cpp inspect estimate hf:${model.id}${tagName}`,
        },
    ];
};
const snippetOllama = (model, filepath) => {
    if (filepath) {
        const quantLabel = (0, gguf_js_1.parseGGUFQuantLabel)(filepath);
        const ollamatag = quantLabel ? `:${quantLabel}` : "";
        return `ollama run hf.co/${model.id}${ollamatag}`;
    }
    return `ollama run hf.co/${model.id}{{OLLAMA_TAG}}`;
};
const snippetLocalAI = (model, filepath) => {
    const command = (binary) => ["# Load and run the model:", `${binary} huggingface://${model.id}/${filepath ?? "{{GGUF_FILE}}"}`].join("\n");
    return [
        {
            title: "Install from binary",
            setup: "curl https://localai.io/install.sh | sh",
            content: command("local-ai run"),
        },
        {
            title: "Use Docker images",
            setup: [
                // prettier-ignore
                "# Pull the image:",
                "docker pull localai/localai:latest-cpu",
            ].join("\n"),
            content: command("docker run -p 8080:8080 --name localai -v $PWD/models:/build/models localai/localai:latest-cpu"),
        },
    ];
};
const snippetVllm = (model) => {
    const messages = (0, inputs_js_1.getModelInputSnippet)(model);
    const runCommandInstruct = `# Call the server using curl:
curl -X POST "http://localhost:8000/v1/chat/completions" \\
	-H "Content-Type: application/json" \\
	--data '{
		"model": "${model.id}",
		"messages": ${(0, common_js_1.stringifyMessages)(messages, {
        indent: "\t\t",
        attributeKeyQuotes: true,
        customContentEscaper: (str) => str.replace(/'/g, "'\\''"),
    })}
	}'`;
    const runCommandNonInstruct = `# Call the server using curl:
curl -X POST "http://localhost:8000/v1/completions" \\
	-H "Content-Type: application/json" \\
	--data '{
		"model": "${model.id}",
		"prompt": "Once upon a time,",
		"max_tokens": 512,
		"temperature": 0.5
	}'`;
    const runCommand = model.tags.includes("conversational") ? runCommandInstruct : runCommandNonInstruct;
    return [
        {
            title: "Install from pip",
            setup: ["# Install vLLM from pip:", "pip install vllm"].join("\n"),
            content: [`# Load and run the model:\nvllm serve "${model.id}"`, runCommand],
        },
        {
            title: "Use Docker images",
            setup: [
                "# Deploy with docker on Linux:",
                `docker run --runtime nvidia --gpus all \\`,
                `	--name my_vllm_container \\`,
                `	-v ~/.cache/huggingface:/root/.cache/huggingface \\`,
                ` 	--env "HUGGING_FACE_HUB_TOKEN=<secret>" \\`,
                `	-p 8000:8000 \\`,
                `	--ipc=host \\`,
                `	vllm/vllm-openai:latest \\`,
                `	--model ${model.id}`,
            ].join("\n"),
            content: [
                `# Load and run the model:\ndocker exec -it my_vllm_container bash -c "vllm serve ${model.id}"`,
                runCommand,
            ],
        },
    ];
};
const snippetTgi = (model) => {
    const runCommand = [
        "# Call the server using curl:",
        `curl -X POST "http://localhost:8000/v1/chat/completions" \\`,
        `	-H "Content-Type: application/json" \\`,
        `	--data '{`,
        `		"model": "${model.id}",`,
        `		"messages": [`,
        `			{"role": "user", "content": "What is the capital of France?"}`,
        `		]`,
        `	}'`,
    ];
    return [
        {
            title: "Use Docker images",
            setup: [
                "# Deploy with docker on Linux:",
                `docker run --gpus all \\`,
                `	-v ~/.cache/huggingface:/root/.cache/huggingface \\`,
                ` 	-e HF_TOKEN="<secret>" \\`,
                `	-p 8000:80 \\`,
                `	ghcr.io/huggingface/text-generation-inference:latest \\`,
                `	--model-id ${model.id}`,
            ].join("\n"),
            content: [runCommand.join("\n")],
        },
    ];
};
/**
 * Add your new local app here.
 *
 * This is open to new suggestions and awesome upcoming apps.
 *
 * /!\ IMPORTANT
 *
 * If possible, you need to support deeplinks and be as cross-platform as possible.
 *
 * Ping the HF team if we can help with anything!
 */
exports.LOCAL_APPS = {
    "llama.cpp": {
        prettyLabel: "llama.cpp",
        docsUrl: "https://github.com/ggerganov/llama.cpp",
        mainTask: "text-generation",
        displayOnModelPage: isLlamaCppGgufModel,
        snippet: snippetLlamacpp,
    },
    "node-llama-cpp": {
        prettyLabel: "node-llama-cpp",
        docsUrl: "https://node-llama-cpp.withcat.ai",
        mainTask: "text-generation",
        displayOnModelPage: isLlamaCppGgufModel,
        snippet: snippetNodeLlamaCppCli,
    },
    vllm: {
        prettyLabel: "vLLM",
        docsUrl: "https://docs.vllm.ai",
        mainTask: "text-generation",
        displayOnModelPage: (model) => (isAwqModel(model) ||
            isGptqModel(model) ||
            isAqlmModel(model) ||
            isMarlinModel(model) ||
            isLlamaCppGgufModel(model) ||
            isTransformersModel(model)) &&
            (model.pipeline_tag === "text-generation" || model.pipeline_tag === "image-text-to-text"),
        snippet: snippetVllm,
    },
    tgi: {
        prettyLabel: "TGI",
        docsUrl: "https://huggingface.co/docs/text-generation-inference/",
        mainTask: "text-generation",
        displayOnModelPage: isTgiModel,
        snippet: snippetTgi,
    },
    lmstudio: {
        prettyLabel: "LM Studio",
        docsUrl: "https://lmstudio.ai",
        mainTask: "text-generation",
        displayOnModelPage: (model) => isLlamaCppGgufModel(model) || isMlxModel(model),
        deeplink: (model, filepath) => new URL(`lmstudio://open_from_hf?model=${model.id}${filepath ? `&file=${filepath}` : ""}`),
    },
    localai: {
        prettyLabel: "LocalAI",
        docsUrl: "https://github.com/mudler/LocalAI",
        mainTask: "text-generation",
        displayOnModelPage: isLlamaCppGgufModel,
        snippet: snippetLocalAI,
    },
    jan: {
        prettyLabel: "Jan",
        docsUrl: "https://jan.ai",
        mainTask: "text-generation",
        displayOnModelPage: isLlamaCppGgufModel,
        deeplink: (model) => new URL(`jan://models/huggingface/${model.id}`),
    },
    backyard: {
        prettyLabel: "Backyard AI",
        docsUrl: "https://backyard.ai",
        mainTask: "text-generation",
        displayOnModelPage: isLlamaCppGgufModel,
        deeplink: (model) => new URL(`https://backyard.ai/hf/model/${model.id}`),
    },
    sanctum: {
        prettyLabel: "Sanctum",
        docsUrl: "https://sanctum.ai",
        mainTask: "text-generation",
        displayOnModelPage: isLlamaCppGgufModel,
        deeplink: (model) => new URL(`sanctum://open_from_hf?model=${model.id}`),
    },
    jellybox: {
        prettyLabel: "Jellybox",
        docsUrl: "https://jellybox.com",
        mainTask: "text-generation",
        displayOnModelPage: (model) => isLlamaCppGgufModel(model) ||
            (model.library_name === "diffusers" &&
                model.tags.includes("safetensors") &&
                (model.pipeline_tag === "text-to-image" || model.tags.includes("lora"))),
        deeplink: (model) => {
            if (isLlamaCppGgufModel(model)) {
                return new URL(`jellybox://llm/models/huggingface/LLM/${model.id}`);
            }
            else if (model.tags.includes("lora")) {
                return new URL(`jellybox://image/models/huggingface/ImageLora/${model.id}`);
            }
            else {
                return new URL(`jellybox://image/models/huggingface/Image/${model.id}`);
            }
        },
    },
    msty: {
        prettyLabel: "Msty",
        docsUrl: "https://msty.app",
        mainTask: "text-generation",
        displayOnModelPage: isLlamaCppGgufModel,
        deeplink: (model) => new URL(`msty://models/search/hf/${model.id}`),
    },
    recursechat: {
        prettyLabel: "RecurseChat",
        docsUrl: "https://recurse.chat",
        mainTask: "text-generation",
        macOSOnly: true,
        displayOnModelPage: isLlamaCppGgufModel,
        deeplink: (model) => new URL(`recursechat://new-hf-gguf-model?hf-model-id=${model.id}`),
    },
    drawthings: {
        prettyLabel: "Draw Things",
        docsUrl: "https://drawthings.ai",
        mainTask: "text-to-image",
        macOSOnly: true,
        displayOnModelPage: (model) => model.library_name === "diffusers" && (model.pipeline_tag === "text-to-image" || model.tags.includes("lora")),
        deeplink: (model) => {
            if (model.tags.includes("lora")) {
                return new URL(`https://drawthings.ai/import/diffusers/pipeline.load_lora_weights?repo_id=${model.id}`);
            }
            else {
                return new URL(`https://drawthings.ai/import/diffusers/pipeline.from_pretrained?repo_id=${model.id}`);
            }
        },
    },
    diffusionbee: {
        prettyLabel: "DiffusionBee",
        docsUrl: "https://diffusionbee.com",
        mainTask: "text-to-image",
        macOSOnly: true,
        displayOnModelPage: (model) => model.library_name === "diffusers" && model.pipeline_tag === "text-to-image",
        deeplink: (model) => new URL(`https://diffusionbee.com/huggingface_import?model_id=${model.id}`),
    },
    joyfusion: {
        prettyLabel: "JoyFusion",
        docsUrl: "https://joyfusion.app",
        mainTask: "text-to-image",
        macOSOnly: true,
        displayOnModelPage: (model) => model.tags.includes("coreml") && model.tags.includes("joyfusion") && model.pipeline_tag === "text-to-image",
        deeplink: (model) => new URL(`https://joyfusion.app/import_from_hf?repo_id=${model.id}`),
    },
    invoke: {
        prettyLabel: "Invoke",
        docsUrl: "https://github.com/invoke-ai/InvokeAI",
        mainTask: "text-to-image",
        displayOnModelPage: (model) => model.library_name === "diffusers" && model.pipeline_tag === "text-to-image",
        deeplink: (model) => new URL(`https://models.invoke.ai/huggingface/${model.id}`),
    },
    ollama: {
        prettyLabel: "Ollama",
        docsUrl: "https://ollama.com",
        mainTask: "text-generation",
        displayOnModelPage: isLlamaCppGgufModel,
        snippet: snippetOllama,
    },
};
