var __defProp = Object.defineProperty;
var __export = (target, all) => {
  for (var name2 in all)
    __defProp(target, name2, { get: all[name2], enumerable: true });
};

// src/tasks/index.ts
var tasks_exports = {};
__export(tasks_exports, {
  audioClassification: () => audioClassification,
  audioToAudio: () => audioToAudio,
  automaticSpeechRecognition: () => automaticSpeechRecognition,
  chatCompletion: () => chatCompletion,
  chatCompletionStream: () => chatCompletionStream,
  documentQuestionAnswering: () => documentQuestionAnswering,
  featureExtraction: () => featureExtraction,
  fillMask: () => fillMask,
  imageClassification: () => imageClassification,
  imageSegmentation: () => imageSegmentation,
  imageToImage: () => imageToImage,
  imageToText: () => imageToText,
  objectDetection: () => objectDetection,
  questionAnswering: () => questionAnswering,
  request: () => request,
  sentenceSimilarity: () => sentenceSimilarity,
  streamingRequest: () => streamingRequest,
  summarization: () => summarization,
  tableQuestionAnswering: () => tableQuestionAnswering,
  tabularClassification: () => tabularClassification,
  tabularRegression: () => tabularRegression,
  textClassification: () => textClassification,
  textGeneration: () => textGeneration,
  textGenerationStream: () => textGenerationStream,
  textToImage: () => textToImage,
  textToSpeech: () => textToSpeech,
  textToVideo: () => textToVideo,
  tokenClassification: () => tokenClassification,
  translation: () => translation,
  visualQuestionAnswering: () => visualQuestionAnswering,
  zeroShotClassification: () => zeroShotClassification,
  zeroShotImageClassification: () => zeroShotImageClassification
});

// src/config.ts
var HF_HUB_URL = "https://huggingface.co";
var HF_ROUTER_URL = "https://router.huggingface.co";

// src/providers/black-forest-labs.ts
var BLACK_FOREST_LABS_AI_API_BASE_URL = "https://api.us1.bfl.ai";
var makeBody = (params) => {
  return params.args;
};
var makeHeaders = (params) => {
  if (params.authMethod === "provider-key") {
    return { "X-Key": `${params.accessToken}` };
  } else {
    return { Authorization: `Bearer ${params.accessToken}` };
  }
};
var makeUrl = (params) => {
  return `${params.baseUrl}/v1/${params.model}`;
};
var BLACK_FOREST_LABS_CONFIG = {
  baseUrl: BLACK_FOREST_LABS_AI_API_BASE_URL,
  makeBody,
  makeHeaders,
  makeUrl
};

// src/providers/cerebras.ts
var CEREBRAS_API_BASE_URL = "https://api.cerebras.ai";
var makeBody2 = (params) => {
  return {
    ...params.args,
    model: params.model
  };
};
var makeHeaders2 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl2 = (params) => {
  return `${params.baseUrl}/v1/chat/completions`;
};
var CEREBRAS_CONFIG = {
  baseUrl: CEREBRAS_API_BASE_URL,
  makeBody: makeBody2,
  makeHeaders: makeHeaders2,
  makeUrl: makeUrl2
};

// src/providers/cohere.ts
var COHERE_API_BASE_URL = "https://api.cohere.com";
var makeBody3 = (params) => {
  return {
    ...params.args,
    model: params.model
  };
};
var makeHeaders3 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl3 = (params) => {
  return `${params.baseUrl}/compatibility/v1/chat/completions`;
};
var COHERE_CONFIG = {
  baseUrl: COHERE_API_BASE_URL,
  makeBody: makeBody3,
  makeHeaders: makeHeaders3,
  makeUrl: makeUrl3
};

// src/providers/fal-ai.ts
var FAL_AI_API_BASE_URL = "https://fal.run";
var makeBody4 = (params) => {
  return params.args;
};
var makeHeaders4 = (params) => {
  return {
    Authorization: params.authMethod === "provider-key" ? `Key ${params.accessToken}` : `Bearer ${params.accessToken}`
  };
};
var makeUrl4 = (params) => {
  return `${params.baseUrl}/${params.model}`;
};
var FAL_AI_CONFIG = {
  baseUrl: FAL_AI_API_BASE_URL,
  makeBody: makeBody4,
  makeHeaders: makeHeaders4,
  makeUrl: makeUrl4
};

// src/providers/fireworks-ai.ts
var FIREWORKS_AI_API_BASE_URL = "https://api.fireworks.ai";
var makeBody5 = (params) => {
  return {
    ...params.args,
    ...params.chatCompletion ? { model: params.model } : void 0
  };
};
var makeHeaders5 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl5 = (params) => {
  if (params.task === "text-generation" && params.chatCompletion) {
    return `${params.baseUrl}/inference/v1/chat/completions`;
  }
  return `${params.baseUrl}/inference`;
};
var FIREWORKS_AI_CONFIG = {
  baseUrl: FIREWORKS_AI_API_BASE_URL,
  makeBody: makeBody5,
  makeHeaders: makeHeaders5,
  makeUrl: makeUrl5
};

// src/providers/hf-inference.ts
var makeBody6 = (params) => {
  return {
    ...params.args,
    ...params.chatCompletion ? { model: params.model } : void 0
  };
};
var makeHeaders6 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl6 = (params) => {
  if (params.task && ["feature-extraction", "sentence-similarity"].includes(params.task)) {
    return `${params.baseUrl}/pipeline/${params.task}/${params.model}`;
  }
  if (params.task === "text-generation" && params.chatCompletion) {
    return `${params.baseUrl}/models/${params.model}/v1/chat/completions`;
  }
  return `${params.baseUrl}/models/${params.model}`;
};
var HF_INFERENCE_CONFIG = {
  baseUrl: `${HF_ROUTER_URL}/hf-inference`,
  makeBody: makeBody6,
  makeHeaders: makeHeaders6,
  makeUrl: makeUrl6
};

// src/providers/hyperbolic.ts
var HYPERBOLIC_API_BASE_URL = "https://api.hyperbolic.xyz";
var makeBody7 = (params) => {
  return {
    ...params.args,
    ...params.task === "text-to-image" ? { model_name: params.model } : { model: params.model }
  };
};
var makeHeaders7 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl7 = (params) => {
  if (params.task === "text-to-image") {
    return `${params.baseUrl}/v1/images/generations`;
  }
  return `${params.baseUrl}/v1/chat/completions`;
};
var HYPERBOLIC_CONFIG = {
  baseUrl: HYPERBOLIC_API_BASE_URL,
  makeBody: makeBody7,
  makeHeaders: makeHeaders7,
  makeUrl: makeUrl7
};

// src/providers/nebius.ts
var NEBIUS_API_BASE_URL = "https://api.studio.nebius.ai";
var makeBody8 = (params) => {
  return {
    ...params.args,
    model: params.model
  };
};
var makeHeaders8 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl8 = (params) => {
  if (params.task === "text-to-image") {
    return `${params.baseUrl}/v1/images/generations`;
  }
  if (params.task === "text-generation") {
    if (params.chatCompletion) {
      return `${params.baseUrl}/v1/chat/completions`;
    }
    return `${params.baseUrl}/v1/completions`;
  }
  return params.baseUrl;
};
var NEBIUS_CONFIG = {
  baseUrl: NEBIUS_API_BASE_URL,
  makeBody: makeBody8,
  makeHeaders: makeHeaders8,
  makeUrl: makeUrl8
};

// src/providers/novita.ts
var NOVITA_API_BASE_URL = "https://api.novita.ai/v3/openai";
var makeBody9 = (params) => {
  return {
    ...params.args,
    ...params.chatCompletion ? { model: params.model } : void 0
  };
};
var makeHeaders9 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl9 = (params) => {
  if (params.task === "text-generation") {
    if (params.chatCompletion) {
      return `${params.baseUrl}/chat/completions`;
    }
    return `${params.baseUrl}/completions`;
  }
  return params.baseUrl;
};
var NOVITA_CONFIG = {
  baseUrl: NOVITA_API_BASE_URL,
  makeBody: makeBody9,
  makeHeaders: makeHeaders9,
  makeUrl: makeUrl9
};

// src/providers/replicate.ts
var REPLICATE_API_BASE_URL = "https://api.replicate.com";
var makeBody10 = (params) => {
  return {
    input: params.args,
    version: params.model.includes(":") ? params.model.split(":")[1] : void 0
  };
};
var makeHeaders10 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}`, Prefer: "wait" };
};
var makeUrl10 = (params) => {
  if (params.model.includes(":")) {
    return `${params.baseUrl}/v1/predictions`;
  }
  return `${params.baseUrl}/v1/models/${params.model}/predictions`;
};
var REPLICATE_CONFIG = {
  baseUrl: REPLICATE_API_BASE_URL,
  makeBody: makeBody10,
  makeHeaders: makeHeaders10,
  makeUrl: makeUrl10
};

// src/providers/sambanova.ts
var SAMBANOVA_API_BASE_URL = "https://api.sambanova.ai";
var makeBody11 = (params) => {
  return {
    ...params.args,
    ...params.chatCompletion ? { model: params.model } : void 0
  };
};
var makeHeaders11 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl11 = (params) => {
  if (params.task === "text-generation" && params.chatCompletion) {
    return `${params.baseUrl}/v1/chat/completions`;
  }
  return params.baseUrl;
};
var SAMBANOVA_CONFIG = {
  baseUrl: SAMBANOVA_API_BASE_URL,
  makeBody: makeBody11,
  makeHeaders: makeHeaders11,
  makeUrl: makeUrl11
};

// src/providers/together.ts
var TOGETHER_API_BASE_URL = "https://api.together.xyz";
var makeBody12 = (params) => {
  return {
    ...params.args,
    model: params.model
  };
};
var makeHeaders12 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl12 = (params) => {
  if (params.task === "text-to-image") {
    return `${params.baseUrl}/v1/images/generations`;
  }
  if (params.task === "text-generation") {
    if (params.chatCompletion) {
      return `${params.baseUrl}/v1/chat/completions`;
    }
    return `${params.baseUrl}/v1/completions`;
  }
  return params.baseUrl;
};
var TOGETHER_CONFIG = {
  baseUrl: TOGETHER_API_BASE_URL,
  makeBody: makeBody12,
  makeHeaders: makeHeaders12,
  makeUrl: makeUrl12
};

// src/providers/openai.ts
var OPENAI_API_BASE_URL = "https://api.openai.com";
var makeBody13 = (params) => {
  if (!params.chatCompletion) {
    throw new Error("OpenAI only supports chat completions.");
  }
  return {
    ...params.args,
    model: params.model
  };
};
var makeHeaders13 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl13 = (params) => {
  if (!params.chatCompletion) {
    throw new Error("OpenAI only supports chat completions.");
  }
  return `${params.baseUrl}/v1/chat/completions`;
};
var OPENAI_CONFIG = {
  baseUrl: OPENAI_API_BASE_URL,
  makeBody: makeBody13,
  makeHeaders: makeHeaders13,
  makeUrl: makeUrl13,
  clientSideRoutingOnly: true
};

// src/lib/isUrl.ts
function isUrl(modelOrUrl) {
  return /^http(s?):/.test(modelOrUrl) || modelOrUrl.startsWith("/");
}

// package.json
var name = "@huggingface/inference";
var version = "3.5.2";

// src/providers/consts.ts
var HARDCODED_MODEL_ID_MAPPING = {
  /**
   * "HF model ID" => "Model ID on Inference Provider's side"
   *
   * Example:
   * "Qwen/Qwen2.5-Coder-32B-Instruct": "Qwen2.5-Coder-32B-Instruct",
   */
  "black-forest-labs": {},
  cerebras: {},
  cohere: {},
  "fal-ai": {},
  "fireworks-ai": {},
  "hf-inference": {},
  hyperbolic: {},
  nebius: {},
  novita: {},
  openai: {},
  replicate: {},
  sambanova: {},
  together: {}
};

// src/lib/getProviderModelId.ts
var inferenceProviderMappingCache = /* @__PURE__ */ new Map();
async function getProviderModelId(params, args, options = {}) {
  if (params.provider === "hf-inference") {
    return params.model;
  }
  if (!options.task) {
    throw new Error("task must be specified when using a third-party provider");
  }
  const task = options.task === "text-generation" && options.chatCompletion ? "conversational" : options.task;
  if (HARDCODED_MODEL_ID_MAPPING[params.provider]?.[params.model]) {
    return HARDCODED_MODEL_ID_MAPPING[params.provider][params.model];
  }
  let inferenceProviderMapping;
  if (inferenceProviderMappingCache.has(params.model)) {
    inferenceProviderMapping = inferenceProviderMappingCache.get(params.model);
  } else {
    inferenceProviderMapping = await (options?.fetch ?? fetch)(
      `${HF_HUB_URL}/api/models/${params.model}?expand[]=inferenceProviderMapping`,
      {
        headers: args.accessToken?.startsWith("hf_") ? { Authorization: `Bearer ${args.accessToken}` } : {}
      }
    ).then((resp) => resp.json()).then((json) => json.inferenceProviderMapping).catch(() => null);
  }
  if (!inferenceProviderMapping) {
    throw new Error(`We have not been able to find inference provider information for model ${params.model}.`);
  }
  const providerMapping = inferenceProviderMapping[params.provider];
  if (providerMapping) {
    if (providerMapping.task !== task) {
      throw new Error(
        `Model ${params.model} is not supported for task ${task} and provider ${params.provider}. Supported task: ${providerMapping.task}.`
      );
    }
    if (providerMapping.status === "staging") {
      console.warn(
        `Model ${params.model} is in staging mode for provider ${params.provider}. Meant for test purposes only.`
      );
    }
    return providerMapping.providerId;
  }
  throw new Error(`Model ${params.model} is not supported provider ${params.provider}.`);
}

// src/lib/makeRequestOptions.ts
var HF_HUB_INFERENCE_PROXY_TEMPLATE = `${HF_ROUTER_URL}/{{PROVIDER}}`;
var tasks = null;
var providerConfigs = {
  "black-forest-labs": BLACK_FOREST_LABS_CONFIG,
  cerebras: CEREBRAS_CONFIG,
  cohere: COHERE_CONFIG,
  "fal-ai": FAL_AI_CONFIG,
  "fireworks-ai": FIREWORKS_AI_CONFIG,
  "hf-inference": HF_INFERENCE_CONFIG,
  hyperbolic: HYPERBOLIC_CONFIG,
  openai: OPENAI_CONFIG,
  nebius: NEBIUS_CONFIG,
  novita: NOVITA_CONFIG,
  replicate: REPLICATE_CONFIG,
  sambanova: SAMBANOVA_CONFIG,
  together: TOGETHER_CONFIG
};
async function makeRequestOptions(args, options) {
  const { accessToken, endpointUrl, provider: maybeProvider, model: maybeModel, ...remainingArgs } = args;
  const provider = maybeProvider ?? "hf-inference";
  const providerConfig = providerConfigs[provider];
  const { includeCredentials, task, chatCompletion: chatCompletion2, signal } = options ?? {};
  if (endpointUrl && provider !== "hf-inference") {
    throw new Error(`Cannot use endpointUrl with a third-party provider.`);
  }
  if (maybeModel && isUrl(maybeModel)) {
    throw new Error(`Model URLs are no longer supported. Use endpointUrl instead.`);
  }
  if (!maybeModel && !task) {
    throw new Error("No model provided, and no task has been specified.");
  }
  if (!providerConfig) {
    throw new Error(`No provider config found for provider ${provider}`);
  }
  if (providerConfig.clientSideRoutingOnly && !maybeModel) {
    throw new Error(`Provider ${provider} requires a model ID to be passed directly.`);
  }
  const hfModel = maybeModel ?? await loadDefaultModel(task);
  const model = providerConfig.clientSideRoutingOnly ? (
    // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
    removeProviderPrefix(maybeModel, provider)
  ) : (
    // For closed-models API providers, one needs to pass the model ID directly (e.g. "gpt-3.5-turbo")
    await getProviderModelId({ model: hfModel, provider }, args, {
      task,
      chatCompletion: chatCompletion2,
      fetch: options?.fetch
    })
  );
  const authMethod = (() => {
    if (providerConfig.clientSideRoutingOnly) {
      if (accessToken && accessToken.startsWith("hf_")) {
        throw new Error(`Provider ${provider} is closed-source and does not support HF tokens.`);
      }
      return "provider-key";
    }
    if (accessToken) {
      return accessToken.startsWith("hf_") ? "hf-token" : "provider-key";
    }
    if (includeCredentials === "include") {
      return "credentials-include";
    }
    return "none";
  })();
  const url = endpointUrl ? chatCompletion2 ? endpointUrl + `/v1/chat/completions` : endpointUrl : providerConfig.makeUrl({
    baseUrl: authMethod !== "provider-key" ? HF_HUB_INFERENCE_PROXY_TEMPLATE.replace("{{PROVIDER}}", provider) : providerConfig.baseUrl,
    model,
    chatCompletion: chatCompletion2,
    task
  });
  const binary = "data" in args && !!args.data;
  const headers = providerConfig.makeHeaders({
    accessToken,
    authMethod
  });
  if (!binary) {
    headers["Content-Type"] = "application/json";
  }
  const ownUserAgent = `${name}/${version}`;
  const userAgent = [ownUserAgent, typeof navigator !== "undefined" ? navigator.userAgent : void 0].filter((x) => x !== void 0).join(" ");
  headers["User-Agent"] = userAgent;
  const body = binary ? args.data : JSON.stringify(
    providerConfig.makeBody({
      args: remainingArgs,
      model,
      task,
      chatCompletion: chatCompletion2
    })
  );
  let credentials;
  if (typeof includeCredentials === "string") {
    credentials = includeCredentials;
  } else if (includeCredentials === true) {
    credentials = "include";
  }
  const info = {
    headers,
    method: "POST",
    body,
    ...credentials ? { credentials } : void 0,
    signal
  };
  return { url, info };
}
async function loadDefaultModel(task) {
  if (!tasks) {
    tasks = await loadTaskInfo();
  }
  const taskInfo = tasks[task];
  if ((taskInfo?.models.length ?? 0) <= 0) {
    throw new Error(`No default model defined for task ${task}, please define the model explicitly.`);
  }
  return taskInfo.models[0].id;
}
async function loadTaskInfo() {
  const res = await fetch(`${HF_HUB_URL}/api/tasks`);
  if (!res.ok) {
    throw new Error("Failed to load tasks definitions from Hugging Face Hub.");
  }
  return await res.json();
}
function removeProviderPrefix(model, provider) {
  if (!model.startsWith(`${provider}/`)) {
    throw new Error(`Models from ${provider} must be prefixed by "${provider}/". Got "${model}".`);
  }
  return model.slice(provider.length + 1);
}

// src/tasks/custom/request.ts
async function request(args, options) {
  const { url, info } = await makeRequestOptions(args, options);
  const response = await (options?.fetch ?? fetch)(url, info);
  if (options?.retry_on_error !== false && response.status === 503) {
    return request(args, options);
  }
  if (!response.ok) {
    const contentType = response.headers.get("Content-Type");
    if (["application/json", "application/problem+json"].some((ct) => contentType?.startsWith(ct))) {
      const output = await response.json();
      if ([400, 422, 404, 500].includes(response.status) && options?.chatCompletion) {
        throw new Error(
          `Server ${args.model} does not seem to support chat completion. Error: ${JSON.stringify(output.error)}`
        );
      }
      if (output.error || output.detail) {
        throw new Error(JSON.stringify(output.error ?? output.detail));
      } else {
        throw new Error(output);
      }
    }
    const message = contentType?.startsWith("text/plain;") ? await response.text() : void 0;
    throw new Error(message ?? "An error occurred while fetching the blob");
  }
  if (response.headers.get("Content-Type")?.startsWith("application/json")) {
    return await response.json();
  }
  return await response.blob();
}

// src/vendor/fetch-event-source/parse.ts
function getLines(onLine) {
  let buffer;
  let position;
  let fieldLength;
  let discardTrailingNewline = false;
  return function onChunk(arr) {
    if (buffer === void 0) {
      buffer = arr;
      position = 0;
      fieldLength = -1;
    } else {
      buffer = concat(buffer, arr);
    }
    const bufLength = buffer.length;
    let lineStart = 0;
    while (position < bufLength) {
      if (discardTrailingNewline) {
        if (buffer[position] === 10 /* NewLine */) {
          lineStart = ++position;
        }
        discardTrailingNewline = false;
      }
      let lineEnd = -1;
      for (; position < bufLength && lineEnd === -1; ++position) {
        switch (buffer[position]) {
          case 58 /* Colon */:
            if (fieldLength === -1) {
              fieldLength = position - lineStart;
            }
            break;
          case 13 /* CarriageReturn */:
            discardTrailingNewline = true;
          case 10 /* NewLine */:
            lineEnd = position;
            break;
        }
      }
      if (lineEnd === -1) {
        break;
      }
      onLine(buffer.subarray(lineStart, lineEnd), fieldLength);
      lineStart = position;
      fieldLength = -1;
    }
    if (lineStart === bufLength) {
      buffer = void 0;
    } else if (lineStart !== 0) {
      buffer = buffer.subarray(lineStart);
      position -= lineStart;
    }
  };
}
function getMessages(onId, onRetry, onMessage) {
  let message = newMessage();
  const decoder = new TextDecoder();
  return function onLine(line, fieldLength) {
    if (line.length === 0) {
      onMessage?.(message);
      message = newMessage();
    } else if (fieldLength > 0) {
      const field = decoder.decode(line.subarray(0, fieldLength));
      const valueOffset = fieldLength + (line[fieldLength + 1] === 32 /* Space */ ? 2 : 1);
      const value = decoder.decode(line.subarray(valueOffset));
      switch (field) {
        case "data":
          message.data = message.data ? message.data + "\n" + value : value;
          break;
        case "event":
          message.event = value;
          break;
        case "id":
          onId(message.id = value);
          break;
        case "retry":
          const retry = parseInt(value, 10);
          if (!isNaN(retry)) {
            onRetry(message.retry = retry);
          }
          break;
      }
    }
  };
}
function concat(a, b) {
  const res = new Uint8Array(a.length + b.length);
  res.set(a);
  res.set(b, a.length);
  return res;
}
function newMessage() {
  return {
    data: "",
    event: "",
    id: "",
    retry: void 0
  };
}

// src/tasks/custom/streamingRequest.ts
async function* streamingRequest(args, options) {
  const { url, info } = await makeRequestOptions({ ...args, stream: true }, options);
  const response = await (options?.fetch ?? fetch)(url, info);
  if (options?.retry_on_error !== false && response.status === 503) {
    return yield* streamingRequest(args, options);
  }
  if (!response.ok) {
    if (response.headers.get("Content-Type")?.startsWith("application/json")) {
      const output = await response.json();
      if ([400, 422, 404, 500].includes(response.status) && options?.chatCompletion) {
        throw new Error(`Server ${args.model} does not seem to support chat completion. Error: ${output.error}`);
      }
      if (typeof output.error === "string") {
        throw new Error(output.error);
      }
      if (output.error && "message" in output.error && typeof output.error.message === "string") {
        throw new Error(output.error.message);
      }
    }
    throw new Error(`Server response contains error: ${response.status}`);
  }
  if (!response.headers.get("content-type")?.startsWith("text/event-stream")) {
    throw new Error(
      `Server does not support event stream content type, it returned ` + response.headers.get("content-type")
    );
  }
  if (!response.body) {
    return;
  }
  const reader = response.body.getReader();
  let events = [];
  const onEvent = (event) => {
    events.push(event);
  };
  const onChunk = getLines(
    getMessages(
      () => {
      },
      () => {
      },
      onEvent
    )
  );
  try {
    while (true) {
      const { done, value } = await reader.read();
      if (done) {
        return;
      }
      onChunk(value);
      for (const event of events) {
        if (event.data.length > 0) {
          if (event.data === "[DONE]") {
            return;
          }
          const data = JSON.parse(event.data);
          if (typeof data === "object" && data !== null && "error" in data) {
            const errorStr = typeof data.error === "string" ? data.error : typeof data.error === "object" && data.error && "message" in data.error && typeof data.error.message === "string" ? data.error.message : JSON.stringify(data.error);
            throw new Error(`Error forwarded from backend: ` + errorStr);
          }
          yield data;
        }
      }
      events = [];
    }
  } finally {
    reader.releaseLock();
  }
}

// src/lib/InferenceOutputError.ts
var InferenceOutputError = class extends TypeError {
  constructor(message) {
    super(
      `Invalid inference output: ${message}. Use the 'request' method with the same parameters to do a custom call with no type checking.`
    );
    this.name = "InferenceOutputError";
  }
};

// src/utils/pick.ts
function pick(o, props) {
  return Object.assign(
    {},
    ...props.map((prop) => {
      if (o[prop] !== void 0) {
        return { [prop]: o[prop] };
      }
    })
  );
}

// src/utils/typedInclude.ts
function typedInclude(arr, v) {
  return arr.includes(v);
}

// src/utils/omit.ts
function omit(o, props) {
  const propsArr = Array.isArray(props) ? props : [props];
  const letsKeep = Object.keys(o).filter((prop) => !typedInclude(propsArr, prop));
  return pick(o, letsKeep);
}

// src/tasks/audio/utils.ts
function preparePayload(args) {
  return "data" in args ? args : {
    ...omit(args, "inputs"),
    data: args.inputs
  };
}

// src/tasks/audio/audioClassification.ts
async function audioClassification(args, options) {
  const payload = preparePayload(args);
  const res = await request(payload, {
    ...options,
    task: "audio-classification"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x.label === "string" && typeof x.score === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{label: string, score: number}>");
  }
  return res;
}

// src/utils/base64FromBytes.ts
function base64FromBytes(arr) {
  if (globalThis.Buffer) {
    return globalThis.Buffer.from(arr).toString("base64");
  } else {
    const bin = [];
    arr.forEach((byte) => {
      bin.push(String.fromCharCode(byte));
    });
    return globalThis.btoa(bin.join(""));
  }
}

// src/tasks/audio/automaticSpeechRecognition.ts
async function automaticSpeechRecognition(args, options) {
  const payload = await buildPayload(args);
  const res = await request(payload, {
    ...options,
    task: "automatic-speech-recognition"
  });
  const isValidOutput = typeof res?.text === "string";
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected {text: string}");
  }
  return res;
}
var FAL_AI_SUPPORTED_BLOB_TYPES = ["audio/mpeg", "audio/mp4", "audio/wav", "audio/x-wav"];
async function buildPayload(args) {
  if (args.provider === "fal-ai") {
    const blob = "data" in args && args.data instanceof Blob ? args.data : "inputs" in args ? args.inputs : void 0;
    const contentType = blob?.type;
    if (!contentType) {
      throw new Error(
        `Unable to determine the input's content-type. Make sure your are passing a Blob when using provider fal-ai.`
      );
    }
    if (!FAL_AI_SUPPORTED_BLOB_TYPES.includes(contentType)) {
      throw new Error(
        `Provider fal-ai does not support blob type ${contentType} - supported content types are: ${FAL_AI_SUPPORTED_BLOB_TYPES.join(
          ", "
        )}`
      );
    }
    const base64audio = base64FromBytes(new Uint8Array(await blob.arrayBuffer()));
    return {
      ..."data" in args ? omit(args, "data") : omit(args, "inputs"),
      audio_url: `data:${contentType};base64,${base64audio}`
    };
  } else {
    return preparePayload(args);
  }
}

// src/tasks/audio/textToSpeech.ts
async function textToSpeech(args, options) {
  const payload = args.provider === "replicate" ? {
    ...omit(args, ["inputs", "parameters"]),
    ...args.parameters,
    text: args.inputs
  } : args;
  const res = await request(payload, {
    ...options,
    task: "text-to-speech"
  });
  if (res instanceof Blob) {
    return res;
  }
  if (res && typeof res === "object") {
    if ("output" in res) {
      if (typeof res.output === "string") {
        const urlResponse = await fetch(res.output);
        const blob = await urlResponse.blob();
        return blob;
      } else if (Array.isArray(res.output)) {
        const urlResponse = await fetch(res.output[0]);
        const blob = await urlResponse.blob();
        return blob;
      }
    }
  }
  throw new InferenceOutputError("Expected Blob or object with output");
}

// src/tasks/audio/audioToAudio.ts
async function audioToAudio(args, options) {
  const payload = preparePayload(args);
  const res = await request(payload, {
    ...options,
    task: "audio-to-audio"
  });
  return validateOutput(res);
}
function validateOutput(output) {
  if (!Array.isArray(output)) {
    throw new InferenceOutputError("Expected Array");
  }
  if (!output.every((elem) => {
    return typeof elem === "object" && elem && "label" in elem && typeof elem.label === "string" && "content-type" in elem && typeof elem["content-type"] === "string" && "blob" in elem && typeof elem.blob === "string";
  })) {
    throw new InferenceOutputError("Expected Array<{label: string, audio: Blob}>");
  }
  return output;
}

// src/tasks/cv/utils.ts
function preparePayload2(args) {
  return "data" in args ? args : { ...omit(args, "inputs"), data: args.inputs };
}

// src/tasks/cv/imageClassification.ts
async function imageClassification(args, options) {
  const payload = preparePayload2(args);
  const res = await request(payload, {
    ...options,
    task: "image-classification"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x.label === "string" && typeof x.score === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{label: string, score: number}>");
  }
  return res;
}

// src/tasks/cv/imageSegmentation.ts
async function imageSegmentation(args, options) {
  const payload = preparePayload2(args);
  const res = await request(payload, {
    ...options,
    task: "image-segmentation"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x.label === "string" && typeof x.mask === "string" && typeof x.score === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{label: string, mask: string, score: number}>");
  }
  return res;
}

// src/tasks/cv/imageToText.ts
async function imageToText(args, options) {
  const payload = preparePayload2(args);
  const res = (await request(payload, {
    ...options,
    task: "image-to-text"
  }))?.[0];
  if (typeof res?.generated_text !== "string") {
    throw new InferenceOutputError("Expected {generated_text: string}");
  }
  return res;
}

// src/tasks/cv/objectDetection.ts
async function objectDetection(args, options) {
  const payload = preparePayload2(args);
  const res = await request(payload, {
    ...options,
    task: "object-detection"
  });
  const isValidOutput = Array.isArray(res) && res.every(
    (x) => typeof x.label === "string" && typeof x.score === "number" && typeof x.box.xmin === "number" && typeof x.box.ymin === "number" && typeof x.box.xmax === "number" && typeof x.box.ymax === "number"
  );
  if (!isValidOutput) {
    throw new InferenceOutputError(
      "Expected Array<{label:string; score:number; box:{xmin:number; ymin:number; xmax:number; ymax:number}}>"
    );
  }
  return res;
}

// src/utils/delay.ts
function delay(ms) {
  return new Promise((resolve) => {
    setTimeout(() => resolve(), ms);
  });
}

// src/tasks/cv/textToImage.ts
function getResponseFormatArg(provider) {
  switch (provider) {
    case "fal-ai":
      return { sync_mode: true };
    case "nebius":
      return { response_format: "b64_json" };
    case "replicate":
      return void 0;
    case "together":
      return { response_format: "base64" };
    default:
      return void 0;
  }
}
async function textToImage(args, options) {
  const payload = !args.provider || args.provider === "hf-inference" || args.provider === "sambanova" ? args : {
    ...omit(args, ["inputs", "parameters"]),
    ...args.parameters,
    ...getResponseFormatArg(args.provider),
    prompt: args.inputs
  };
  const res = await request(payload, {
    ...options,
    task: "text-to-image"
  });
  if (res && typeof res === "object") {
    if (args.provider === "black-forest-labs" && "polling_url" in res && typeof res.polling_url === "string") {
      return await pollBflResponse(res.polling_url, options?.outputType);
    }
    if (args.provider === "fal-ai" && "images" in res && Array.isArray(res.images) && res.images[0].url) {
      if (options?.outputType === "url") {
        return res.images[0].url;
      } else {
        const image = await fetch(res.images[0].url);
        return await image.blob();
      }
    }
    if (args.provider === "hyperbolic" && "images" in res && Array.isArray(res.images) && res.images[0] && typeof res.images[0].image === "string") {
      if (options?.outputType === "url") {
        return `data:image/jpeg;base64,${res.images[0].image}`;
      }
      const base64Response = await fetch(`data:image/jpeg;base64,${res.images[0].image}`);
      return await base64Response.blob();
    }
    if ("data" in res && Array.isArray(res.data) && res.data[0].b64_json) {
      const base64Data = res.data[0].b64_json;
      if (options?.outputType === "url") {
        return `data:image/jpeg;base64,${base64Data}`;
      }
      const base64Response = await fetch(`data:image/jpeg;base64,${base64Data}`);
      return await base64Response.blob();
    }
    if ("output" in res && Array.isArray(res.output)) {
      if (options?.outputType === "url") {
        return res.output[0];
      }
      const urlResponse = await fetch(res.output[0]);
      const blob = await urlResponse.blob();
      return blob;
    }
  }
  const isValidOutput = res && res instanceof Blob;
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Blob");
  }
  if (options?.outputType === "url") {
    const b64 = await res.arrayBuffer().then((buf) => Buffer.from(buf).toString("base64"));
    return `data:image/jpeg;base64,${b64}`;
  }
  return res;
}
async function pollBflResponse(url, outputType) {
  const urlObj = new URL(url);
  for (let step = 0; step < 5; step++) {
    await delay(1e3);
    console.debug(`Polling Black Forest Labs API for the result... ${step + 1}/5`);
    urlObj.searchParams.set("attempt", step.toString(10));
    const resp = await fetch(urlObj, { headers: { "Content-Type": "application/json" } });
    if (!resp.ok) {
      throw new InferenceOutputError("Failed to fetch result from black forest labs API");
    }
    const payload = await resp.json();
    if (typeof payload === "object" && payload && "status" in payload && typeof payload.status === "string" && payload.status === "Ready" && "result" in payload && typeof payload.result === "object" && payload.result && "sample" in payload.result && typeof payload.result.sample === "string") {
      if (outputType === "url") {
        return payload.result.sample;
      }
      const image = await fetch(payload.result.sample);
      return await image.blob();
    }
  }
  throw new InferenceOutputError("Failed to fetch result from black forest labs API");
}

// src/tasks/cv/imageToImage.ts
async function imageToImage(args, options) {
  let reqArgs;
  if (!args.parameters) {
    reqArgs = {
      accessToken: args.accessToken,
      model: args.model,
      data: args.inputs
    };
  } else {
    reqArgs = {
      ...args,
      inputs: base64FromBytes(
        new Uint8Array(args.inputs instanceof ArrayBuffer ? args.inputs : await args.inputs.arrayBuffer())
      )
    };
  }
  const res = await request(reqArgs, {
    ...options,
    task: "image-to-image"
  });
  const isValidOutput = res && res instanceof Blob;
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Blob");
  }
  return res;
}

// src/tasks/cv/zeroShotImageClassification.ts
async function preparePayload3(args) {
  if (args.inputs instanceof Blob) {
    return {
      ...args,
      inputs: {
        image: base64FromBytes(new Uint8Array(await args.inputs.arrayBuffer()))
      }
    };
  } else {
    return {
      ...args,
      inputs: {
        image: base64FromBytes(
          new Uint8Array(
            args.inputs.image instanceof ArrayBuffer ? args.inputs.image : await args.inputs.image.arrayBuffer()
          )
        )
      }
    };
  }
}
async function zeroShotImageClassification(args, options) {
  const payload = await preparePayload3(args);
  const res = await request(payload, {
    ...options,
    task: "zero-shot-image-classification"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x.label === "string" && typeof x.score === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{label: string, score: number}>");
  }
  return res;
}

// src/tasks/cv/textToVideo.ts
var SUPPORTED_PROVIDERS = ["fal-ai", "replicate"];
async function textToVideo(args, options) {
  if (!args.provider || !typedInclude(SUPPORTED_PROVIDERS, args.provider)) {
    throw new Error(
      `textToVideo inference is only supported for the following providers: ${SUPPORTED_PROVIDERS.join(", ")}`
    );
  }
  const payload = args.provider === "fal-ai" || args.provider === "replicate" ? { ...omit(args, ["inputs", "parameters"]), ...args.parameters, prompt: args.inputs } : args;
  const res = await request(payload, {
    ...options,
    task: "text-to-video"
  });
  if (args.provider === "fal-ai") {
    const isValidOutput = typeof res === "object" && !!res && "video" in res && typeof res.video === "object" && !!res.video && "url" in res.video && typeof res.video.url === "string" && isUrl(res.video.url);
    if (!isValidOutput) {
      throw new InferenceOutputError("Expected { video: { url: string } }");
    }
    const urlResponse = await fetch(res.video.url);
    return await urlResponse.blob();
  } else {
    const isValidOutput = typeof res === "object" && !!res && "output" in res && typeof res.output === "string" && isUrl(res.output);
    if (!isValidOutput) {
      throw new InferenceOutputError("Expected { output: string }");
    }
    const urlResponse = await fetch(res.output);
    return await urlResponse.blob();
  }
}

// src/tasks/nlp/featureExtraction.ts
async function featureExtraction(args, options) {
  const res = await request(args, {
    ...options,
    task: "feature-extraction"
  });
  let isValidOutput = true;
  const isNumArrayRec = (arr, maxDepth, curDepth = 0) => {
    if (curDepth > maxDepth)
      return false;
    if (arr.every((x) => Array.isArray(x))) {
      return arr.every((x) => isNumArrayRec(x, maxDepth, curDepth + 1));
    } else {
      return arr.every((x) => typeof x === "number");
    }
  };
  isValidOutput = Array.isArray(res) && isNumArrayRec(res, 3, 0);
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<number[][][] | number[][] | number[] | number>");
  }
  return res;
}

// src/tasks/nlp/fillMask.ts
async function fillMask(args, options) {
  const res = await request(args, {
    ...options,
    task: "fill-mask"
  });
  const isValidOutput = Array.isArray(res) && res.every(
    (x) => typeof x.score === "number" && typeof x.sequence === "string" && typeof x.token === "number" && typeof x.token_str === "string"
  );
  if (!isValidOutput) {
    throw new InferenceOutputError(
      "Expected Array<{score: number, sequence: string, token: number, token_str: string}>"
    );
  }
  return res;
}

// src/tasks/nlp/questionAnswering.ts
async function questionAnswering(args, options) {
  const res = await request(args, {
    ...options,
    task: "question-answering"
  });
  const isValidOutput = Array.isArray(res) ? res.every(
    (elem) => typeof elem === "object" && !!elem && typeof elem.answer === "string" && typeof elem.end === "number" && typeof elem.score === "number" && typeof elem.start === "number"
  ) : typeof res === "object" && !!res && typeof res.answer === "string" && typeof res.end === "number" && typeof res.score === "number" && typeof res.start === "number";
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{answer: string, end: number, score: number, start: number}>");
  }
  return Array.isArray(res) ? res[0] : res;
}

// src/tasks/nlp/sentenceSimilarity.ts
async function sentenceSimilarity(args, options) {
  const res = await request(prepareInput(args), {
    ...options,
    task: "sentence-similarity"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected number[]");
  }
  return res;
}
function prepareInput(args) {
  return {
    ...omit(args, ["inputs", "parameters"]),
    inputs: { ...omit(args.inputs, "sourceSentence") },
    parameters: { source_sentence: args.inputs.sourceSentence, ...args.parameters }
  };
}

// src/tasks/nlp/summarization.ts
async function summarization(args, options) {
  const res = await request(args, {
    ...options,
    task: "summarization"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x?.summary_text === "string");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{summary_text: string}>");
  }
  return res?.[0];
}

// src/tasks/nlp/tableQuestionAnswering.ts
async function tableQuestionAnswering(args, options) {
  const res = await request(args, {
    ...options,
    task: "table-question-answering"
  });
  const isValidOutput = Array.isArray(res) ? res.every((elem) => validate(elem)) : validate(res);
  if (!isValidOutput) {
    throw new InferenceOutputError(
      "Expected {aggregator: string, answer: string, cells: string[], coordinates: number[][]}"
    );
  }
  return Array.isArray(res) ? res[0] : res;
}
function validate(elem) {
  return typeof elem === "object" && !!elem && "aggregator" in elem && typeof elem.aggregator === "string" && "answer" in elem && typeof elem.answer === "string" && "cells" in elem && Array.isArray(elem.cells) && elem.cells.every((x) => typeof x === "string") && "coordinates" in elem && Array.isArray(elem.coordinates) && elem.coordinates.every(
    (coord) => Array.isArray(coord) && coord.every((x) => typeof x === "number")
  );
}

// src/tasks/nlp/textClassification.ts
async function textClassification(args, options) {
  const res = (await request(args, {
    ...options,
    task: "text-classification"
  }))?.[0];
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x?.label === "string" && typeof x.score === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{label: string, score: number}>");
  }
  return res;
}

// src/utils/toArray.ts
function toArray(obj) {
  if (Array.isArray(obj)) {
    return obj;
  }
  return [obj];
}

// src/tasks/nlp/textGeneration.ts
async function textGeneration(args, options) {
  if (args.provider === "together") {
    args.prompt = args.inputs;
    const raw = await request(args, {
      ...options,
      task: "text-generation"
    });
    const isValidOutput = typeof raw === "object" && "choices" in raw && Array.isArray(raw?.choices) && typeof raw?.model === "string";
    if (!isValidOutput) {
      throw new InferenceOutputError("Expected ChatCompletionOutput");
    }
    const completion = raw.choices[0];
    return {
      generated_text: completion.text
    };
  } else if (args.provider === "hyperbolic") {
    const payload = {
      messages: [{ content: args.inputs, role: "user" }],
      ...args.parameters ? {
        max_tokens: args.parameters.max_new_tokens,
        ...omit(args.parameters, "max_new_tokens")
      } : void 0,
      ...omit(args, ["inputs", "parameters"])
    };
    const raw = await request(payload, {
      ...options,
      task: "text-generation"
    });
    const isValidOutput = typeof raw === "object" && "choices" in raw && Array.isArray(raw?.choices) && typeof raw?.model === "string";
    if (!isValidOutput) {
      throw new InferenceOutputError("Expected ChatCompletionOutput");
    }
    const completion = raw.choices[0];
    return {
      generated_text: completion.message.content
    };
  } else {
    const res = toArray(
      await request(args, {
        ...options,
        task: "text-generation"
      })
    );
    const isValidOutput = Array.isArray(res) && res.every((x) => "generated_text" in x && typeof x?.generated_text === "string");
    if (!isValidOutput) {
      throw new InferenceOutputError("Expected Array<{generated_text: string}>");
    }
    return res?.[0];
  }
}

// src/tasks/nlp/textGenerationStream.ts
async function* textGenerationStream(args, options) {
  yield* streamingRequest(args, {
    ...options,
    task: "text-generation"
  });
}

// src/tasks/nlp/tokenClassification.ts
async function tokenClassification(args, options) {
  const res = toArray(
    await request(args, {
      ...options,
      task: "token-classification"
    })
  );
  const isValidOutput = Array.isArray(res) && res.every(
    (x) => typeof x.end === "number" && typeof x.entity_group === "string" && typeof x.score === "number" && typeof x.start === "number" && typeof x.word === "string"
  );
  if (!isValidOutput) {
    throw new InferenceOutputError(
      "Expected Array<{end: number, entity_group: string, score: number, start: number, word: string}>"
    );
  }
  return res;
}

// src/tasks/nlp/translation.ts
async function translation(args, options) {
  const res = await request(args, {
    ...options,
    task: "translation"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x?.translation_text === "string");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected type Array<{translation_text: string}>");
  }
  return res?.length === 1 ? res?.[0] : res;
}

// src/tasks/nlp/zeroShotClassification.ts
async function zeroShotClassification(args, options) {
  const res = toArray(
    await request(args, {
      ...options,
      task: "zero-shot-classification"
    })
  );
  const isValidOutput = Array.isArray(res) && res.every(
    (x) => Array.isArray(x.labels) && x.labels.every((_label) => typeof _label === "string") && Array.isArray(x.scores) && x.scores.every((_score) => typeof _score === "number") && typeof x.sequence === "string"
  );
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{labels: string[], scores: number[], sequence: string}>");
  }
  return res;
}

// src/tasks/nlp/chatCompletion.ts
async function chatCompletion(args, options) {
  const res = await request(args, {
    ...options,
    task: "text-generation",
    chatCompletion: true
  });
  const isValidOutput = typeof res === "object" && Array.isArray(res?.choices) && typeof res?.created === "number" && typeof res?.id === "string" && typeof res?.model === "string" && /// Together.ai and Nebius do not output a system_fingerprint
  (res.system_fingerprint === void 0 || res.system_fingerprint === null || typeof res.system_fingerprint === "string") && typeof res?.usage === "object";
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected ChatCompletionOutput");
  }
  return res;
}

// src/tasks/nlp/chatCompletionStream.ts
async function* chatCompletionStream(args, options) {
  yield* streamingRequest(args, {
    ...options,
    task: "text-generation",
    chatCompletion: true
  });
}

// src/tasks/multimodal/documentQuestionAnswering.ts
async function documentQuestionAnswering(args, options) {
  const reqArgs = {
    ...args,
    inputs: {
      question: args.inputs.question,
      // convert Blob or ArrayBuffer to base64
      image: base64FromBytes(new Uint8Array(await args.inputs.image.arrayBuffer()))
    }
  };
  const res = toArray(
    await request(reqArgs, {
      ...options,
      task: "document-question-answering"
    })
  );
  const isValidOutput = Array.isArray(res) && res.every(
    (elem) => typeof elem === "object" && !!elem && typeof elem?.answer === "string" && (typeof elem.end === "number" || typeof elem.end === "undefined") && (typeof elem.score === "number" || typeof elem.score === "undefined") && (typeof elem.start === "number" || typeof elem.start === "undefined")
  );
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{answer: string, end?: number, score?: number, start?: number}>");
  }
  return res[0];
}

// src/tasks/multimodal/visualQuestionAnswering.ts
async function visualQuestionAnswering(args, options) {
  const reqArgs = {
    ...args,
    inputs: {
      question: args.inputs.question,
      // convert Blob or ArrayBuffer to base64
      image: base64FromBytes(new Uint8Array(await args.inputs.image.arrayBuffer()))
    }
  };
  const res = await request(reqArgs, {
    ...options,
    task: "visual-question-answering"
  });
  const isValidOutput = Array.isArray(res) && res.every(
    (elem) => typeof elem === "object" && !!elem && typeof elem?.answer === "string" && typeof elem.score === "number"
  );
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{answer: string, score: number}>");
  }
  return res[0];
}

// src/tasks/tabular/tabularRegression.ts
async function tabularRegression(args, options) {
  const res = await request(args, {
    ...options,
    task: "tabular-regression"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected number[]");
  }
  return res;
}

// src/tasks/tabular/tabularClassification.ts
async function tabularClassification(args, options) {
  const res = await request(args, {
    ...options,
    task: "tabular-classification"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected number[]");
  }
  return res;
}

// src/InferenceClient.ts
var InferenceClient = class {
  accessToken;
  defaultOptions;
  constructor(accessToken = "", defaultOptions = {}) {
    this.accessToken = accessToken;
    this.defaultOptions = defaultOptions;
    for (const [name2, fn] of Object.entries(tasks_exports)) {
      Object.defineProperty(this, name2, {
        enumerable: false,
        value: (params, options) => (
          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          fn({ ...params, accessToken }, { ...defaultOptions, ...options })
        )
      });
    }
  }
  /**
   * Returns copy of InferenceClient tied to a specified endpoint.
   */
  endpoint(endpointUrl) {
    return new InferenceClientEndpoint(endpointUrl, this.accessToken, this.defaultOptions);
  }
};
var InferenceClientEndpoint = class {
  constructor(endpointUrl, accessToken = "", defaultOptions = {}) {
    accessToken;
    defaultOptions;
    for (const [name2, fn] of Object.entries(tasks_exports)) {
      Object.defineProperty(this, name2, {
        enumerable: false,
        value: (params, options) => (
          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          fn({ ...params, accessToken, endpointUrl }, { ...defaultOptions, ...options })
        )
      });
    }
  }
};
var HfInference = class extends InferenceClient {
};

// src/types.ts
var INFERENCE_PROVIDERS = [
  "black-forest-labs",
  "cerebras",
  "cohere",
  "fal-ai",
  "fireworks-ai",
  "hf-inference",
  "hyperbolic",
  "nebius",
  "novita",
  "openai",
  "replicate",
  "sambanova",
  "together"
];

// src/snippets/index.ts
var snippets_exports = {};
__export(snippets_exports, {
  curl: () => curl_exports,
  js: () => js_exports,
  python: () => python_exports
});

// src/snippets/curl.ts
var curl_exports = {};
__export(curl_exports, {
  curlSnippets: () => curlSnippets,
  getCurlInferenceSnippet: () => getCurlInferenceSnippet,
  snippetBasic: () => snippetBasic,
  snippetFile: () => snippetFile,
  snippetTextGeneration: () => snippetTextGeneration,
  snippetZeroShotClassification: () => snippetZeroShotClassification
});
import { HF_HUB_INFERENCE_PROXY_TEMPLATE as HF_HUB_INFERENCE_PROXY_TEMPLATE2 } from "@huggingface/tasks";
import {
  getModelInputSnippet,
  stringifyGenerationConfig,
  stringifyMessages
} from "@huggingface/tasks";
var snippetBasic = (model, accessToken, provider) => {
  if (provider !== "hf-inference") {
    return [];
  }
  return [
    {
      client: "curl",
      content: `curl https://router.huggingface.co/hf-inference/models/${model.id} \\
	-X POST \\
	-d '{"inputs": ${getModelInputSnippet(model, true)}}' \\
	-H 'Content-Type: application/json' \\
	-H 'Authorization: Bearer ${accessToken || `{API_TOKEN}`}'`
    }
  ];
};
var snippetTextGeneration = (model, accessToken, provider, providerModelId, opts) => {
  if (model.tags.includes("conversational")) {
    const baseUrl = provider === "hf-inference" ? `https://router.huggingface.co/hf-inference/models/${model.id}/v1/chat/completions` : HF_HUB_INFERENCE_PROXY_TEMPLATE2.replace("{{PROVIDER}}", provider) + "/v1/chat/completions";
    const modelId = providerModelId ?? model.id;
    const streaming = opts?.streaming ?? true;
    const exampleMessages = getModelInputSnippet(model);
    const messages = opts?.messages ?? exampleMessages;
    const config = {
      ...opts?.temperature ? { temperature: opts.temperature } : void 0,
      max_tokens: opts?.max_tokens ?? 500,
      ...opts?.top_p ? { top_p: opts.top_p } : void 0
    };
    return [
      {
        client: "curl",
        content: `curl '${baseUrl}' \\
-H 'Authorization: Bearer ${accessToken || `{API_TOKEN}`}' \\
-H 'Content-Type: application/json' \\
--data '{
    "model": "${modelId}",
    "messages": ${stringifyMessages(messages, {
          indent: "	",
          attributeKeyQuotes: true,
          customContentEscaper: (str) => str.replace(/'/g, "'\\''")
        })},
    ${stringifyGenerationConfig(config, {
          indent: "\n    ",
          attributeKeyQuotes: true,
          attributeValueConnector: ": "
        })}
    "stream": ${!!streaming}
}'`
      }
    ];
  } else {
    return snippetBasic(model, accessToken, provider);
  }
};
var snippetZeroShotClassification = (model, accessToken, provider) => {
  if (provider !== "hf-inference") {
    return [];
  }
  return [
    {
      client: "curl",
      content: `curl https://router.huggingface.co/hf-inference/models/${model.id} \\
	-X POST \\
	-d '{"inputs": ${getModelInputSnippet(model, true)}, "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
	-H 'Content-Type: application/json' \\
	-H 'Authorization: Bearer ${accessToken || `{API_TOKEN}`}'`
    }
  ];
};
var snippetFile = (model, accessToken, provider) => {
  if (provider !== "hf-inference") {
    return [];
  }
  return [
    {
      client: "curl",
      content: `curl https://router.huggingface.co/hf-inference/models/${model.id} \\
	-X POST \\
	--data-binary '@${getModelInputSnippet(model, true, true)}' \\
	-H 'Authorization: Bearer ${accessToken || `{API_TOKEN}`}'`
    }
  ];
};
var curlSnippets = {
  // Same order as in tasks/src/pipelines.ts
  "text-classification": snippetBasic,
  "token-classification": snippetBasic,
  "table-question-answering": snippetBasic,
  "question-answering": snippetBasic,
  "zero-shot-classification": snippetZeroShotClassification,
  translation: snippetBasic,
  summarization: snippetBasic,
  "feature-extraction": snippetBasic,
  "text-generation": snippetTextGeneration,
  "image-text-to-text": snippetTextGeneration,
  "text2text-generation": snippetBasic,
  "fill-mask": snippetBasic,
  "sentence-similarity": snippetBasic,
  "automatic-speech-recognition": snippetFile,
  "text-to-image": snippetBasic,
  "text-to-speech": snippetBasic,
  "text-to-audio": snippetBasic,
  "audio-to-audio": snippetFile,
  "audio-classification": snippetFile,
  "image-classification": snippetFile,
  "image-to-text": snippetFile,
  "object-detection": snippetFile,
  "image-segmentation": snippetFile
};
function getCurlInferenceSnippet(model, accessToken, provider, providerModelId, opts) {
  return model.pipeline_tag && model.pipeline_tag in curlSnippets ? curlSnippets[model.pipeline_tag]?.(model, accessToken, provider, providerModelId, opts) ?? [] : [];
}

// src/snippets/python.ts
var python_exports = {};
__export(python_exports, {
  getPythonInferenceSnippet: () => getPythonInferenceSnippet
});
import { openAIbaseUrl } from "@huggingface/tasks";
import {
  getModelInputSnippet as getModelInputSnippet2,
  stringifyGenerationConfig as stringifyGenerationConfig2,
  stringifyMessages as stringifyMessages2
} from "@huggingface/tasks";
var HFH_INFERENCE_CLIENT_METHODS = {
  "audio-classification": "audio_classification",
  "audio-to-audio": "audio_to_audio",
  "automatic-speech-recognition": "automatic_speech_recognition",
  "text-to-speech": "text_to_speech",
  "image-classification": "image_classification",
  "image-segmentation": "image_segmentation",
  "image-to-image": "image_to_image",
  "image-to-text": "image_to_text",
  "object-detection": "object_detection",
  "text-to-image": "text_to_image",
  "text-to-video": "text_to_video",
  "zero-shot-image-classification": "zero_shot_image_classification",
  "document-question-answering": "document_question_answering",
  "visual-question-answering": "visual_question_answering",
  "feature-extraction": "feature_extraction",
  "fill-mask": "fill_mask",
  "question-answering": "question_answering",
  "sentence-similarity": "sentence_similarity",
  summarization: "summarization",
  "table-question-answering": "table_question_answering",
  "text-classification": "text_classification",
  "text-generation": "text_generation",
  "token-classification": "token_classification",
  translation: "translation",
  "zero-shot-classification": "zero_shot_classification",
  "tabular-classification": "tabular_classification",
  "tabular-regression": "tabular_regression"
};
var snippetImportInferenceClient = (accessToken, provider) => `from huggingface_hub import InferenceClient

client = InferenceClient(
    provider="${provider}",
    api_key="${accessToken || "{API_TOKEN}"}",
)`;
var snippetConversational = (model, accessToken, provider, providerModelId, opts) => {
  const streaming = opts?.streaming ?? true;
  const exampleMessages = getModelInputSnippet2(model);
  const messages = opts?.messages ?? exampleMessages;
  const messagesStr = stringifyMessages2(messages, { attributeKeyQuotes: true });
  const config = {
    ...opts?.temperature ? { temperature: opts.temperature } : void 0,
    max_tokens: opts?.max_tokens ?? 500,
    ...opts?.top_p ? { top_p: opts.top_p } : void 0
  };
  const configStr = stringifyGenerationConfig2(config, {
    indent: "\n	",
    attributeValueConnector: "="
  });
  if (streaming) {
    return [
      {
        client: "huggingface_hub",
        content: `${snippetImportInferenceClient(accessToken, provider)}

messages = ${messagesStr}

stream = client.chat.completions.create(
	model="${model.id}", 
	messages=messages, 
	${configStr}
	stream=True,
)

for chunk in stream:
    print(chunk.choices[0].delta.content, end="")`
      },
      {
        client: "openai",
        content: `from openai import OpenAI

client = OpenAI(
	base_url="${openAIbaseUrl(provider)}",
	api_key="${accessToken || "{API_TOKEN}"}"
)

messages = ${messagesStr}

stream = client.chat.completions.create(
    model="${providerModelId ?? model.id}", 
	messages=messages, 
	${configStr}
	stream=True
)

for chunk in stream:
	print(chunk.choices[0].delta.content, end="")`
      }
    ];
  } else {
    return [
      {
        client: "huggingface_hub",
        content: `${snippetImportInferenceClient(accessToken, provider)}

messages = ${messagesStr}

completion = client.chat.completions.create(
    model="${model.id}", 
	messages=messages, 
	${configStr}
)

print(completion.choices[0].message)`
      },
      {
        client: "openai",
        content: `from openai import OpenAI

client = OpenAI(
	base_url="${openAIbaseUrl(provider)}",
	api_key="${accessToken || "{API_TOKEN}"}"
)

messages = ${messagesStr}

completion = client.chat.completions.create(
	model="${providerModelId ?? model.id}", 
	messages=messages, 
	${configStr}
)

print(completion.choices[0].message)`
      }
    ];
  }
};
var snippetZeroShotClassification2 = (model) => {
  return [
    {
      client: "requests",
      content: `def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
    "inputs": ${getModelInputSnippet2(model)},
    "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
})`
    }
  ];
};
var snippetZeroShotImageClassification = (model) => {
  return [
    {
      client: "requests",
      content: `def query(data):
	with open(data["image_path"], "rb") as f:
		img = f.read()
	payload={
		"parameters": data["parameters"],
		"inputs": base64.b64encode(img).decode("utf-8")
	}
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
	"image_path": ${getModelInputSnippet2(model)},
	"parameters": {"candidate_labels": ["cat", "dog", "llama"]},
})`
    }
  ];
};
var snippetBasic2 = (model, accessToken, provider) => {
  return [
    ...model.pipeline_tag && model.pipeline_tag in HFH_INFERENCE_CLIENT_METHODS ? [
      {
        client: "huggingface_hub",
        content: `${snippetImportInferenceClient(accessToken, provider)}

result = client.${HFH_INFERENCE_CLIENT_METHODS[model.pipeline_tag]}(
	inputs=${getModelInputSnippet2(model)},
	model="${model.id}",
)

print(result)
`
      }
    ] : [],
    {
      client: "requests",
      content: `def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
output = query({
	"inputs": ${getModelInputSnippet2(model)},
})`
    }
  ];
};
var snippetFile2 = (model) => {
  return [
    {
      client: "requests",
      content: `def query(filename):
	with open(filename, "rb") as f:
		data = f.read()
	response = requests.post(API_URL, headers=headers, data=data)
	return response.json()

output = query(${getModelInputSnippet2(model)})`
    }
  ];
};
var snippetTextToImage = (model, accessToken, provider, providerModelId) => {
  return [
    {
      client: "huggingface_hub",
      content: `${snippetImportInferenceClient(accessToken, provider)}

# output is a PIL.Image object
image = client.text_to_image(
	${getModelInputSnippet2(model)},
	model="${model.id}",
)`
    },
    ...provider === "fal-ai" ? [
      {
        client: "fal-client",
        content: `import fal_client

result = fal_client.subscribe(
	"${providerModelId ?? model.id}",
	arguments={
		"prompt": ${getModelInputSnippet2(model)},
	},
)
print(result)
`
      }
    ] : [],
    ...provider === "hf-inference" ? [
      {
        client: "requests",
        content: `def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.content

image_bytes = query({
	"inputs": ${getModelInputSnippet2(model)},
})

# You can access the image with PIL.Image for example
import io
from PIL import Image
image = Image.open(io.BytesIO(image_bytes))`
      }
    ] : []
  ];
};
var snippetTextToVideo = (model, accessToken, provider) => {
  return ["fal-ai", "replicate"].includes(provider) ? [
    {
      client: "huggingface_hub",
      content: `${snippetImportInferenceClient(accessToken, provider)}

video = client.text_to_video(
	${getModelInputSnippet2(model)},
	model="${model.id}",
)`
    }
  ] : [];
};
var snippetTabular = (model) => {
  return [
    {
      client: "requests",
      content: `def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.content

response = query({
	"inputs": {"data": ${getModelInputSnippet2(model)}},
})`
    }
  ];
};
var snippetTextToAudio = (model) => {
  if (model.library_name === "transformers") {
    return [
      {
        client: "requests",
        content: `def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.content

audio_bytes = query({
	"inputs": ${getModelInputSnippet2(model)},
})
# You can access the audio with IPython.display for example
from IPython.display import Audio
Audio(audio_bytes)`
      }
    ];
  } else {
    return [
      {
        client: "requests",
        content: `def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
audio, sampling_rate = query({
	"inputs": ${getModelInputSnippet2(model)},
})
# You can access the audio with IPython.display for example
from IPython.display import Audio
Audio(audio, rate=sampling_rate)`
      }
    ];
  }
};
var snippetAutomaticSpeechRecognition = (model, accessToken, provider) => {
  return [
    {
      client: "huggingface_hub",
      content: `${snippetImportInferenceClient(accessToken, provider)}
output = client.automatic_speech_recognition(${getModelInputSnippet2(model)}, model="${model.id}")`
    },
    snippetFile2(model)[0]
  ];
};
var snippetDocumentQuestionAnswering = (model, accessToken, provider) => {
  const inputsAsStr = getModelInputSnippet2(model);
  const inputsAsObj = JSON.parse(inputsAsStr);
  return [
    {
      client: "huggingface_hub",
      content: `${snippetImportInferenceClient(accessToken, provider)}
output = client.document_question_answering(
    "${inputsAsObj.image}",
	question="${inputsAsObj.question}",
	model="${model.id}",
)`
    },
    {
      client: "requests",
      content: `def query(payload):
	with open(payload["image"], "rb") as f:
		img = f.read()
		payload["image"] = base64.b64encode(img).decode("utf-8")
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
    "inputs": ${inputsAsStr},
})`
    }
  ];
};
var snippetImageToImage = (model, accessToken, provider) => {
  const inputsAsStr = getModelInputSnippet2(model);
  const inputsAsObj = JSON.parse(inputsAsStr);
  return [
    {
      client: "huggingface_hub",
      content: `${snippetImportInferenceClient(accessToken, provider)}
# output is a PIL.Image object
image = client.image_to_image(
    "${inputsAsObj.image}",
    prompt="${inputsAsObj.prompt}",
    model="${model.id}",
)`
    },
    {
      client: "requests",
      content: `def query(payload):
	with open(payload["inputs"], "rb") as f:
		img = f.read()
		payload["inputs"] = base64.b64encode(img).decode("utf-8")
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.content

image_bytes = query({
	"inputs": "${inputsAsObj.image}",
	"parameters": {"prompt": "${inputsAsObj.prompt}"},
})

# You can access the image with PIL.Image for example
import io
from PIL import Image
image = Image.open(io.BytesIO(image_bytes))`
    }
  ];
};
var pythonSnippets = {
  // Same order as in tasks/src/pipelines.ts
  "text-classification": snippetBasic2,
  "token-classification": snippetBasic2,
  "table-question-answering": snippetBasic2,
  "question-answering": snippetBasic2,
  "zero-shot-classification": snippetZeroShotClassification2,
  translation: snippetBasic2,
  summarization: snippetBasic2,
  "feature-extraction": snippetBasic2,
  "text-generation": snippetBasic2,
  "text2text-generation": snippetBasic2,
  "image-text-to-text": snippetConversational,
  "fill-mask": snippetBasic2,
  "sentence-similarity": snippetBasic2,
  "automatic-speech-recognition": snippetAutomaticSpeechRecognition,
  "text-to-image": snippetTextToImage,
  "text-to-video": snippetTextToVideo,
  "text-to-speech": snippetTextToAudio,
  "text-to-audio": snippetTextToAudio,
  "audio-to-audio": snippetFile2,
  "audio-classification": snippetFile2,
  "image-classification": snippetFile2,
  "tabular-regression": snippetTabular,
  "tabular-classification": snippetTabular,
  "object-detection": snippetFile2,
  "image-segmentation": snippetFile2,
  "document-question-answering": snippetDocumentQuestionAnswering,
  "image-to-text": snippetFile2,
  "image-to-image": snippetImageToImage,
  "zero-shot-image-classification": snippetZeroShotImageClassification
};
function getPythonInferenceSnippet(model, accessToken, provider, providerModelId, opts) {
  if (model.tags.includes("conversational")) {
    return snippetConversational(model, accessToken, provider, providerModelId, opts);
  } else {
    const snippets = model.pipeline_tag && model.pipeline_tag in pythonSnippets ? pythonSnippets[model.pipeline_tag]?.(model, accessToken, provider, providerModelId) ?? [] : [];
    return snippets.map((snippet) => {
      return {
        ...snippet,
        content: addImportsToSnippet(snippet.content, model, accessToken)
      };
    });
  }
}
var addImportsToSnippet = (snippet, model, accessToken) => {
  if (snippet.includes("requests")) {
    snippet = `import requests

API_URL = "https://router.huggingface.co/hf-inference/models/${model.id}"
headers = {"Authorization": ${accessToken ? `"Bearer ${accessToken}"` : `f"Bearer {API_TOKEN}"`}}

${snippet}`;
  }
  if (snippet.includes("base64")) {
    snippet = `import base64
${snippet}`;
  }
  return snippet;
};

// src/snippets/js.ts
var js_exports = {};
__export(js_exports, {
  getJsInferenceSnippet: () => getJsInferenceSnippet,
  jsSnippets: () => jsSnippets,
  snippetAutomaticSpeechRecognition: () => snippetAutomaticSpeechRecognition2,
  snippetBasic: () => snippetBasic3,
  snippetFile: () => snippetFile3,
  snippetTextGeneration: () => snippetTextGeneration2,
  snippetTextToAudio: () => snippetTextToAudio2,
  snippetTextToImage: () => snippetTextToImage2,
  snippetTextToVideo: () => snippetTextToVideo2,
  snippetZeroShotClassification: () => snippetZeroShotClassification3
});
import { openAIbaseUrl as openAIbaseUrl2 } from "@huggingface/tasks";
import {
  getModelInputSnippet as getModelInputSnippet3,
  stringifyGenerationConfig as stringifyGenerationConfig3,
  stringifyMessages as stringifyMessages3
} from "@huggingface/tasks";
var HFJS_METHODS = {
  "text-classification": "textClassification",
  "token-classification": "tokenClassification",
  "table-question-answering": "tableQuestionAnswering",
  "question-answering": "questionAnswering",
  translation: "translation",
  summarization: "summarization",
  "feature-extraction": "featureExtraction",
  "text-generation": "textGeneration",
  "text2text-generation": "textGeneration",
  "fill-mask": "fillMask",
  "sentence-similarity": "sentenceSimilarity"
};
var snippetBasic3 = (model, accessToken, provider) => {
  return [
    ...model.pipeline_tag && model.pipeline_tag in HFJS_METHODS ? [
      {
        client: "huggingface.js",
        content: `import { InferenceClient } from "@huggingface/inference";

const client = new InferenceClient("${accessToken || `{API_TOKEN}`}");

const output = await client.${HFJS_METHODS[model.pipeline_tag]}({
	model: "${model.id}",
	inputs: ${getModelInputSnippet3(model)},
	provider: "${provider}",
});

console.log(output);
`
      }
    ] : [],
    {
      client: "fetch",
      content: `async function query(data) {
	const response = await fetch(
		"https://router.huggingface.co/hf-inference/models/${model.id}",
		{
			headers: {
				Authorization: "Bearer ${accessToken || `{API_TOKEN}`}",
				"Content-Type": "application/json",
			},
			method: "POST",
			body: JSON.stringify(data),
		}
	);
	const result = await response.json();
	return result;
}

query({"inputs": ${getModelInputSnippet3(model)}}).then((response) => {
	console.log(JSON.stringify(response));
});`
    }
  ];
};
var snippetTextGeneration2 = (model, accessToken, provider, providerModelId, opts) => {
  if (model.tags.includes("conversational")) {
    const streaming = opts?.streaming ?? true;
    const exampleMessages = getModelInputSnippet3(model);
    const messages = opts?.messages ?? exampleMessages;
    const messagesStr = stringifyMessages3(messages, { indent: "	" });
    const config = {
      ...opts?.temperature ? { temperature: opts.temperature } : void 0,
      max_tokens: opts?.max_tokens ?? 500,
      ...opts?.top_p ? { top_p: opts.top_p } : void 0
    };
    const configStr = stringifyGenerationConfig3(config, {
      indent: "\n	",
      attributeValueConnector: ": "
    });
    if (streaming) {
      return [
        {
          client: "huggingface.js",
          content: `import { InferenceClient } from "@huggingface/inference";

const client = new InferenceClient("${accessToken || `{API_TOKEN}`}");

let out = "";

const stream = client.chatCompletionStream({
	model: "${model.id}",
	messages: ${messagesStr},
	provider: "${provider}",
	${configStr}
});

for await (const chunk of stream) {
	if (chunk.choices && chunk.choices.length > 0) {
		const newContent = chunk.choices[0].delta.content;
		out += newContent;
		console.log(newContent);
	}  
}`
        },
        {
          client: "openai",
          content: `import { OpenAI } from "openai";

const client = new OpenAI({
	baseURL: "${openAIbaseUrl2(provider)}",
	apiKey: "${accessToken || `{API_TOKEN}`}"
});

let out = "";

const stream = await client.chat.completions.create({
	model: "${providerModelId ?? model.id}",
	messages: ${messagesStr},
	${configStr}
	stream: true,
});

for await (const chunk of stream) {
	if (chunk.choices && chunk.choices.length > 0) {
		const newContent = chunk.choices[0].delta.content;
		out += newContent;
		console.log(newContent);
	}  
}`
        }
      ];
    } else {
      return [
        {
          client: "huggingface.js",
          content: `import { InferenceClient } from "@huggingface/inference";

const client = new InferenceClient("${accessToken || `{API_TOKEN}`}");

const chatCompletion = await client.chatCompletion({
	model: "${model.id}",
	messages: ${messagesStr},
	provider: "${provider}",
	${configStr}
});

console.log(chatCompletion.choices[0].message);
`
        },
        {
          client: "openai",
          content: `import { OpenAI } from "openai";

const client = new OpenAI({
	baseURL: "${openAIbaseUrl2(provider)}",
	apiKey: "${accessToken || `{API_TOKEN}`}"
});

const chatCompletion = await client.chat.completions.create({
	model: "${providerModelId ?? model.id}",
	messages: ${messagesStr},
	${configStr}
});

console.log(chatCompletion.choices[0].message);
`
        }
      ];
    }
  } else {
    return snippetBasic3(model, accessToken, provider);
  }
};
var snippetZeroShotClassification3 = (model, accessToken) => {
  return [
    {
      client: "fetch",
      content: `async function query(data) {
			const response = await fetch(
				"https://router.huggingface.co/hf-inference/models/${model.id}",
				{
					headers: {
						Authorization: "Bearer ${accessToken || `{API_TOKEN}`}",
						"Content-Type": "application/json",
					},
					method: "POST",
					body: JSON.stringify(data),
				}
			);
			const result = await response.json();
			return result;
		}
		
		query({"inputs": ${getModelInputSnippet3(
        model
      )}, "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}).then((response) => {
			console.log(JSON.stringify(response));
		});`
    }
  ];
};
var snippetTextToImage2 = (model, accessToken, provider) => {
  return [
    {
      client: "huggingface.js",
      content: `import { InferenceClient } from "@huggingface/inference";

const client = new InferenceClient("${accessToken || `{API_TOKEN}`}");

const image = await client.textToImage({
	model: "${model.id}",
	inputs: ${getModelInputSnippet3(model)},
	parameters: { num_inference_steps: 5 },
	provider: "${provider}",
});
/// Use the generated image (it's a Blob)
`
    },
    ...provider === "hf-inference" ? [
      {
        client: "fetch",
        content: `async function query(data) {
	const response = await fetch(
		"https://router.huggingface.co/hf-inference/models/${model.id}",
		{
			headers: {
				Authorization: "Bearer ${accessToken || `{API_TOKEN}`}",
				"Content-Type": "application/json",
			},
			method: "POST",
			body: JSON.stringify(data),
		}
	);
	const result = await response.blob();
	return result;
}
query({"inputs": ${getModelInputSnippet3(model)}}).then((response) => {
	// Use image
});`
      }
    ] : []
  ];
};
var snippetTextToVideo2 = (model, accessToken, provider) => {
  return ["fal-ai", "replicate"].includes(provider) ? [
    {
      client: "huggingface.js",
      content: `import { InferenceClient } from "@huggingface/inference";

const client = new InferenceClient("${accessToken || `{API_TOKEN}`}");

const video = await client.textToVideo({
	model: "${model.id}",
	provider: "${provider}",
	inputs: ${getModelInputSnippet3(model)},
	parameters: { num_inference_steps: 5 },
});
// Use the generated video (it's a Blob)
`
    }
  ] : [];
};
var snippetTextToAudio2 = (model, accessToken, provider) => {
  if (provider !== "hf-inference") {
    return [];
  }
  const commonSnippet = `async function query(data) {
		const response = await fetch(
			"https://router.huggingface.co/hf-inference/models/${model.id}",
			{
				headers: {
					Authorization: "Bearer ${accessToken || `{API_TOKEN}`}",
					"Content-Type": "application/json",
				},
				method: "POST",
				body: JSON.stringify(data),
			}
		);`;
  if (model.library_name === "transformers") {
    return [
      {
        client: "fetch",
        content: commonSnippet + `
			const result = await response.blob();
			return result;
		}
		query({"inputs": ${getModelInputSnippet3(model)}}).then((response) => {
			// Returns a byte object of the Audio wavform. Use it directly!
		});`
      }
    ];
  } else {
    return [
      {
        client: "fetch",
        content: commonSnippet + `
			const result = await response.json();
			return result;
		}
		
		query({"inputs": ${getModelInputSnippet3(model)}}).then((response) => {
			console.log(JSON.stringify(response));
		});`
      }
    ];
  }
};
var snippetAutomaticSpeechRecognition2 = (model, accessToken, provider) => {
  return [
    {
      client: "huggingface.js",
      content: `import { InferenceClient } from "@huggingface/inference";

const client = new InferenceClient("${accessToken || `{API_TOKEN}`}");

const data = fs.readFileSync(${getModelInputSnippet3(model)});

const output = await client.automaticSpeechRecognition({
	data,
	model: "${model.id}",
	provider: "${provider}",
});

console.log(output);
`
    },
    ...provider === "hf-inference" ? snippetFile3(model, accessToken, provider) : []
  ];
};
var snippetFile3 = (model, accessToken, provider) => {
  if (provider !== "hf-inference") {
    return [];
  }
  return [
    {
      client: "fetch",
      content: `async function query(filename) {
	const data = fs.readFileSync(filename);
	const response = await fetch(
		"https://router.huggingface.co/hf-inference/models/${model.id}",
		{
			headers: {
				Authorization: "Bearer ${accessToken || `{API_TOKEN}`}",
				"Content-Type": "application/json",
			},
			method: "POST",
			body: data,
		}
	);
	const result = await response.json();
	return result;
}

query(${getModelInputSnippet3(model)}).then((response) => {
	console.log(JSON.stringify(response));
});`
    }
  ];
};
var jsSnippets = {
  // Same order as in tasks/src/pipelines.ts
  "text-classification": snippetBasic3,
  "token-classification": snippetBasic3,
  "table-question-answering": snippetBasic3,
  "question-answering": snippetBasic3,
  "zero-shot-classification": snippetZeroShotClassification3,
  translation: snippetBasic3,
  summarization: snippetBasic3,
  "feature-extraction": snippetBasic3,
  "text-generation": snippetTextGeneration2,
  "image-text-to-text": snippetTextGeneration2,
  "text2text-generation": snippetBasic3,
  "fill-mask": snippetBasic3,
  "sentence-similarity": snippetBasic3,
  "automatic-speech-recognition": snippetAutomaticSpeechRecognition2,
  "text-to-image": snippetTextToImage2,
  "text-to-video": snippetTextToVideo2,
  "text-to-speech": snippetTextToAudio2,
  "text-to-audio": snippetTextToAudio2,
  "audio-to-audio": snippetFile3,
  "audio-classification": snippetFile3,
  "image-classification": snippetFile3,
  "image-to-text": snippetFile3,
  "object-detection": snippetFile3,
  "image-segmentation": snippetFile3
};
function getJsInferenceSnippet(model, accessToken, provider, providerModelId, opts) {
  return model.pipeline_tag && model.pipeline_tag in jsSnippets ? jsSnippets[model.pipeline_tag]?.(model, accessToken, provider, providerModelId, opts) ?? [] : [];
}
export {
  HfInference,
  INFERENCE_PROVIDERS,
  InferenceClient,
  InferenceClientEndpoint,
  InferenceOutputError,
  audioClassification,
  audioToAudio,
  automaticSpeechRecognition,
  chatCompletion,
  chatCompletionStream,
  documentQuestionAnswering,
  featureExtraction,
  fillMask,
  imageClassification,
  imageSegmentation,
  imageToImage,
  imageToText,
  objectDetection,
  questionAnswering,
  request,
  sentenceSimilarity,
  snippets_exports as snippets,
  streamingRequest,
  summarization,
  tableQuestionAnswering,
  tabularClassification,
  tabularRegression,
  textClassification,
  textGeneration,
  textGenerationStream,
  textToImage,
  textToSpeech,
  textToVideo,
  tokenClassification,
  translation,
  visualQuestionAnswering,
  zeroShotClassification,
  zeroShotImageClassification
};
